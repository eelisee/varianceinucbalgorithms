{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC-UCB Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "class BanditAlgorithm:\n",
    "    \"\"\"\n",
    "    A class to represent a bandit algorithm and manage its results.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the bandit algorithm.\n",
    "    results : list\n",
    "        A list to store results of the algorithm's performance over iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.results = []\n",
    "\n",
    "    def add_result(self, param, iteration, total_reward, suboptimal_arms_count, total_regret, zeros_count, ones_count):\n",
    "        self.results.append([param, iteration, total_reward, suboptimal_arms_count, round(total_regret, 2), zeros_count, ones_count])\n",
    "\n",
    "    def save_results_to_csv(self, filename):\n",
    "        with open(filename, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Timestep', 'Iteration', 'Total Reward', 'Suboptimal Arms', 'Total Regret', 'Zeros Count', 'Ones Count'])\n",
    "            for result in self.results:\n",
    "                writer.writerow(result)\n",
    "\n",
    "    def calculate_average_results(self):\n",
    "        avg_results = {}\n",
    "        for result in self.results:\n",
    "            param = result[0]\n",
    "            if param not in avg_results:\n",
    "                avg_results[param] = [0, 0, 0, 0, 0]\n",
    "            avg_results[param][0] += result[2]  # Total Reward\n",
    "            avg_results[param][1] += result[3]  # Suboptimal Arms Count\n",
    "            avg_results[param][2] += result[4]  # Total Regret\n",
    "            avg_results[param][3] += result[5]  # Zeros Count\n",
    "            avg_results[param][4] += result[6]  # Ones Count\n",
    "        \n",
    "        for param in avg_results:\n",
    "            avg_results[param] = [param] + [x / 100 for x in avg_results[param]]\n",
    "        return list(avg_results.values())\n",
    "\n",
    "def PAC_UCB_simulation(arm_means, total_steps, c=1, b=1, q=1.3, beta=0.05):\n",
    "    \"\"\"\n",
    "    Simulates the epsilon-greedy algorithm over given time horizons.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    algorithm : BanditAlgorithm\n",
    "        The bandit algorithm instance to store the results.\n",
    "    num_arms : in\n",
    "        The length of the arm_means array.\n",
    "    total_steps : list\n",
    "        A list of time horizons at which to record the results.\n",
    "    c : float\n",
    "        A parameter for the choice of c.\n",
    "    b : float\n",
    "        A parameter for the choice of b.\n",
    "    q : float\n",
    "        A parameter for the choice of q.\n",
    "    beta : float\n",
    "        A parameter for the choice of beta.\n",
    "    \n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where keys are time horizons and values are tuples of (total_reward, suboptimal_arms_count, total_regret, zeros_count, ones_count).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    num_arms = len(arm_means)\n",
    "    rewards = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    sum_of_squares = np.zeros(num_arms)\n",
    "    total_reward = 0\n",
    "    suboptimal_arms_count = 0\n",
    "    total_regret = 0\n",
    "    zeros_count = 0\n",
    "    ones_count = 0\n",
    "    regret = np.zeros(total_steps)\n",
    "    suboptimal_arms = np.zeros(total_steps, dtype=int)\n",
    "    total_rewards = np.zeros(total_steps)\n",
    "    zeros_counts = np.zeros(total_steps, dtype=int)\n",
    "    ones_counts = np.zeros(total_steps,  dtype=int)\n",
    "\n",
    "    for t in range(1, total_steps + 1):\n",
    "        if any(counts < 1):\n",
    "            arm = np.argmin(counts)\n",
    "        else:\n",
    "            PAC = np.zeros(num_arms)\n",
    "            for k in range(num_arms):\n",
    "                if counts[k] > 0:\n",
    "                    mean_reward = rewards[k] / counts[k]\n",
    "                    exploration = max(np.log(num_arms * (counts[k] ** q) / beta), 2)\n",
    "                    variance = (sum_of_squares[k] - counts[k] * (mean_reward ** 2)) / counts[k]\n",
    "                    PAC[k] = mean_reward + np.sqrt((2 * variance * exploration) / counts[k]) + c * (3 * b * exploration / counts[k])\n",
    "            arm = np.argmax(PAC)\n",
    "        \n",
    "        reward = np.random.binomial(1, arm_means[arm])\n",
    "        counts[arm] += 1\n",
    "        rewards[arm] += reward\n",
    "        sum_of_squares[arm] += reward ** 2\n",
    "        total_reward += reward\n",
    "        total_rewards[t - 1] = total_reward\n",
    "\n",
    "        if reward == 0:\n",
    "            zeros_count += 1\n",
    "        else:\n",
    "            ones_count += 1\n",
    "\n",
    "        zeros_counts[t - 1] = zeros_count\n",
    "        ones_counts[t - 1] = ones_count\n",
    "\n",
    "        if arm != np.argmax(arm_means):\n",
    "            suboptimal_arms_count += 1\n",
    "            total_regret += np.max(arm_means) - arm_means[arm]\n",
    "\n",
    "        regret[t - 1] = total_regret\n",
    "        suboptimal_arms[t - 1] = suboptimal_arms_count\n",
    "\n",
    "    return {\n",
    "        \"total_rewards\": total_rewards,\n",
    "        \"suboptimal_arms\": suboptimal_arms,\n",
    "        \"regret\": regret,\n",
    "        \"zeros_counts\": zeros_counts,\n",
    "        \"ones_counts\": ones_counts\n",
    "    }\n",
    "\n",
    "def general_simulation(algorithm, arm_means, parameters, strategy_fn):\n",
    "    \"\"\"\n",
    "    Runs a general simulation for the specified bandit algorithm over given parameters and arm means using a provided simulation function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    algorithm : BanditAlgorithm\n",
    "        The bandit algorithm instance to store the results.\n",
    "    parameters : list\n",
    "        A list of parameters (timesteps) for which to record the results.\n",
    "    arm_means : numpy.ndarray\n",
    "        The true means of each arm.\n",
    "    simulation_func : function\n",
    "        The simulation function to run for the algorithm. This function should accept the same parameters as ETC_simulation and return results in a similar format.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    max_time_horizon = max(parameters)\n",
    "    num_arms = len(arm_means)\n",
    "\n",
    "    for iteration in range(1, 101):\n",
    "        results = strategy_fn(arm_means, max_time_horizon)\n",
    "\n",
    "        for param in parameters:\n",
    "            total_reward = results[\"total_rewards\"][param - 1]\n",
    "            suboptimal_arms_count = results[\"suboptimal_arms\"][param - 1]\n",
    "            total_regret = results[\"regret\"][param - 1]\n",
    "            zeros_count = results[\"zeros_counts\"][param - 1]\n",
    "            ones_count = results[\"ones_counts\"][param - 1]\n",
    "\n",
    "            algorithm.add_result(param, iteration, total_reward, suboptimal_arms_count, total_regret, zeros_count, ones_count)\n",
    "\n",
    "\n",
    "time_horizons = [2, 3, 100, 200, 2000, 10000, 20000, 40000, 60000, 80000, 100000]\n",
    "\n",
    "\n",
    "algorithms = [\n",
    "    BanditAlgorithm(\"7_PAC-UCB\"),\n",
    "]\n",
    "\n",
    "\n",
    "arm_means = np.array([0.495, 0.5])\n",
    "\n",
    "# Perform simulation and save results\n",
    "results_path = r'C:/Users/canis/OneDrive\\Dokumente/uni/uni-surface/FSS 2024/BA/bachelorarbeit_vrlfg/BA/github/BA_code/2_algorithms_results'\n",
    "for algorithm in algorithms:\n",
    "    general_simulation(algorithm, arm_means, time_horizons, PAC_UCB_simulation)\n",
    "    algorithm.save_results_to_csv(f'{results_path}\\{algorithm.name}_results_subopt_ver3.csv')\n",
    "    avg_results = algorithm.calculate_average_results()\n",
    "    with open(f'{results_path}\\{algorithm.name}_average_results_subopt_ver3.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Timestep', 'Average Total Reward', 'Average Suboptimal Arms', 'Average Regret', 'Average Zeros Count', 'Average Ones Count'])\n",
    "        for result in avg_results:\n",
    "            writer.writerow(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
