{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epsilon-Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m results_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/canis/OneDrive/Dokumente/uni/uni-surface/FSS 2024/BA/bachelorarbeit_vrlfg/BA/github/BA_code/2_algorithms_results\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algorithm \u001b[38;5;129;01min\u001b[39;00m algorithms:\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mgeneral_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.495\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_horizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_greedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     algorithm\u001b[38;5;241m.\u001b[39msave_results_to_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgorithm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_results_subopt_ver3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    142\u001b[0m     avg_results \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mcalculate_average_results()\n",
      "Cell \u001b[1;32mIn[5], line 61\u001b[0m, in \u001b[0;36mgeneral_simulation\u001b[1;34m(algorithm, arm_means, parameters, strategy_fn, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m num_arms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(arm_means)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m---> 61\u001b[0m     results \u001b[38;5;241m=\u001b[39m strategy_fn(arm_means, num_arms, max_time_horizon, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[0;32m     64\u001b[0m         total_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m][:param])\n",
      "Cell \u001b[1;32mIn[5], line 116\u001b[0m, in \u001b[0;36mepsilon_greedy\u001b[1;34m(arm_means, num_arms, total_steps, epsilon)\u001b[0m\n\u001b[0;32m    113\u001b[0m Q[arm] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (reward \u001b[38;5;241m-\u001b[39m Q[arm]) \u001b[38;5;241m/\u001b[39m N[arm]  \u001b[38;5;66;03m# Update Q-value incrementally\u001b[39;00m\n\u001b[0;32m    115\u001b[0m regret[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(arm_means) \u001b[38;5;241m-\u001b[39m arm_means[arm]\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arm \u001b[38;5;241m!=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43marm_means\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m     suboptimal_arms[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\canis\\OneDrive\\Dokumente\\uni\\uni-surface\\FSS 2024\\BA\\bachelorarbeit_vrlfg\\BA\\github\\BA_code\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:1298\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\canis\\OneDrive\\Dokumente\\uni\\uni-surface\\FSS 2024\\BA\\bachelorarbeit_vrlfg\\BA\\github\\BA_code\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class BanditAlgorithm:\n",
    "    \"\"\"\n",
    "    A class to represent a bandit algorithm and manage its results.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the bandit algorithm.\n",
    "    results : list\n",
    "        A list to store results of the algorithm's performance over iterations.\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.results = []\n",
    "\n",
    "    def add_result(self, timestep, iteration, total_reward, suboptimal_arms, total_regret, zeros_count, ones_count):\n",
    "        self.results.append((timestep, iteration, total_reward, suboptimal_arms, round(total_regret, 2), np.sum(zeros_count), np.sum(ones_count)))\n",
    "\n",
    "    def save_results_to_csv(self, filename):\n",
    "        self.results.sort(key=lambda x: (x[1], x[0]))\n",
    "        with open(filename, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Timestep', 'Iteration', 'Total Reward', 'Suboptimal Arms', 'Total Regret', 'Zeros Count', 'Ones Count'])\n",
    "            for result in self.results:\n",
    "                writer.writerow(result)\n",
    "\n",
    "    def calculate_average_results(self):\n",
    "        time_steps = sorted(set(result[0] for result in self.results))\n",
    "        avg_results = []\n",
    "        for timestep in time_steps:\n",
    "            total_reward_sum = 0\n",
    "            suboptimal_arms_sum = 0\n",
    "            regret_sum = 0\n",
    "            zeros_count_sum = 0\n",
    "            ones_count_sum = 0\n",
    "            count = 0\n",
    "            for result in self.results:\n",
    "                if result[0] == timestep:\n",
    "                    total_reward_sum += result[2]\n",
    "                    suboptimal_arms_sum += result[3]\n",
    "                    regret_sum += result[4]\n",
    "                    zeros_count_sum += result[5]\n",
    "                    ones_count_sum += result[6]\n",
    "                    count += 1\n",
    "            avg_total_reward = total_reward_sum / count if count > 0 else 0\n",
    "            avg_suboptimal_arms = suboptimal_arms_sum / count if count > 0 else 0\n",
    "            avg_regret = regret_sum / count if count > 0 else 0\n",
    "            avg_zeros_count = zeros_count_sum / count if count > 0 else 0\n",
    "            avg_ones_count = ones_count_sum / count if count > 0 else 0\n",
    "            avg_results.append((timestep, avg_total_reward, avg_suboptimal_arms, avg_regret, avg_zeros_count, avg_ones_count))\n",
    "        return avg_results\n",
    "\n",
    "def general_simulation(algorithm, arm_means, parameters, strategy_fn, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs a general simulation for the specified bandit algorithm over given parameters and arm means using a provided simulation function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    algorithm : BanditAlgorithm\n",
    "        The bandit algorithm instance to store the results.\n",
    "    parameters : list\n",
    "        A list of parameters (timesteps) for which to record the results.\n",
    "    arm_means : numpy.ndarray\n",
    "        The true means of each arm.\n",
    "    simulation_func : function\n",
    "        The simulation function to run for the algorithm. This function should accept the same parameters as ETC_simulation and return results in a similar format.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    max_time_horizon = max(parameters)\n",
    "    num_arms = len(arm_means)\n",
    "    \n",
    "    for iteration in range(1, 101):\n",
    "        results = strategy_fn(arm_means, num_arms, max_time_horizon, **kwargs)\n",
    "        \n",
    "        for param in parameters:\n",
    "            total_reward = np.sum(results[\"rewards\"][:param])\n",
    "            suboptimal_arms_count = np.sum(results[\"suboptimal_arms\"][:param])\n",
    "            total_regret = np.sum(results[\"regret\"][:param])\n",
    "            zeros_count = np.sum(results[\"zeros_count\"][:param])\n",
    "            ones_count = np.sum(results[\"ones_count\"][:param])\n",
    "            \n",
    "            algorithm.add_result(param, iteration, total_reward, suboptimal_arms_count, total_regret, zeros_count, ones_count)\n",
    "\n",
    "def epsilon_greedy(arm_means, num_arms, total_steps, epsilon):\n",
    "    \"\"\"\n",
    "    Simulates the epsilon-greedy algorithm over given time horizons.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    algorithm : BanditAlgorithm\n",
    "        The bandit algorithm instance to store the results.\n",
    "    num_arms : in\n",
    "        The length of the arm_means array.\n",
    "    total_steps : list\n",
    "        A list of time horizons at which to record the results.\n",
    "    epsilon : float\n",
    "        A parameter for the choice of epsilon\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where keys are time horizons and values are tuples of (total_reward, suboptimal_arms_count, total_regret, zeros_count, ones_count).\n",
    "    \"\"\"\n",
    "\n",
    "    Q = np.zeros(num_arms)\n",
    "    N = np.zeros(num_arms)\n",
    "    rewards = np.zeros(total_steps)\n",
    "    suboptimal_arms = np.zeros(total_steps, dtype=int)\n",
    "    regret = np.zeros(total_steps)\n",
    "    zeros_count = np.zeros(total_steps, dtype=int)\n",
    "    ones_count = np.zeros(total_steps, dtype=int)\n",
    "\n",
    "    for t in range(total_steps):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Exploration\n",
    "            arm = np.random.choice(num_arms)\n",
    "        else:\n",
    "            # Exploitation\n",
    "            arm = np.argmax(Q)\n",
    "\n",
    "        reward = np.random.binomial(1, arm_means[arm])\n",
    "        rewards[t] = reward\n",
    "        N[arm] += 1\n",
    "        Q[arm] += (reward - Q[arm]) / N[arm]  # Update Q-value incrementally\n",
    "        \n",
    "        regret[t] = np.max(arm_means) - arm_means[arm]\n",
    "        if arm != np.argmax(arm_means):\n",
    "            suboptimal_arms[t] = 1\n",
    "        if reward == 0:\n",
    "            zeros_count[t] = 1\n",
    "        else:\n",
    "            ones_count[t] = 1\n",
    "\n",
    "    return {\n",
    "        \"rewards\": rewards,\n",
    "        \"suboptimal_arms\": suboptimal_arms,\n",
    "        \"regret\": regret,\n",
    "        \"zeros_count\": zeros_count,\n",
    "        \"ones_count\": ones_count\n",
    "    }\n",
    "\n",
    "time_horizons = [2, 3, 100, 200, 2000, 10000, 20000, 40000, 60000, 80000, 100000]\n",
    "\n",
    "algorithms = [\n",
    "    BanditAlgorithm(\"2_Greedy\"),\n",
    "]\n",
    "\n",
    "# Perform simulation and save results\n",
    "results_path = r'C:/Users/canis/OneDrive/Dokumente/uni/uni-surface/FSS 2024/BA/bachelorarbeit_vrlfg/BA/github/BA_code/2_algorithms_results'\n",
    "for algorithm in algorithms:\n",
    "    general_simulation(algorithm, np.array([0.495, 0.5]), time_horizons, epsilon_greedy, epsilon=0.05)\n",
    "    algorithm.save_results_to_csv(f'{results_path}/{algorithm.name}_results_subopt_ver3.csv')\n",
    "    avg_results = algorithm.calculate_average_results()\n",
    "    with open(f'{results_path}/{algorithm.name}_average_results_subopt_ver3.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Timestep', 'Average Total Reward', 'Average Suboptimal Arms', 'Average Regret', 'Average Zeros Count', 'Average Ones Count'])\n",
    "        for result in avg_results:\n",
    "            writer.writerow(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
